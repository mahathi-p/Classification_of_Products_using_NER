{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to create a NER model for electronic products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The products used are:\n",
    "\n",
    "- Laptops\n",
    "- Monitors\n",
    "- HardDisks\n",
    "- Printers\n",
    "- Shredders (not included in this code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load data\n",
    "def create_trainData (csv_file,product):\n",
    "    \n",
    "    ## loading data with the columns names as entities\n",
    "    data = pd.read_csv(csv_file,index_col=None)\n",
    "    \n",
    "    ## the entities\n",
    "    cols = data.columns\n",
    "    \n",
    "    ## number of entities\n",
    "    num_ent = len(data.columns)\n",
    "    ent_list = list(np.arange(num_ent))\n",
    "    \n",
    "    ## jumble indices of entities to create a phrase\n",
    "    prod_name = [] # list of all product names\n",
    "    prod_ann = [] # list of all the annotations\n",
    "    for i in range(len(data)): # loop for each laptop\n",
    "        idx_list = random.sample(ent_list,num_ent) # shuffling indices\n",
    "        cont = []\n",
    "        ann = []\n",
    "        ann_idx = 0 # pointer for annotating \n",
    "        for j in range(num_ent): # creating the jumbled product name\n",
    "            col_num = idx_list[j] # column number according jumbled column index\n",
    "            val = data.iloc[i,col_num] # value of the entity \n",
    "            cont.append(str(val)) # appending list of entities into a single list\n",
    "            ann.append((ann_idx, len(str(val))+ ann_idx, cols[col_num])) # annotations and entity name\n",
    "            ann_idx = ann_idx + len(str(val)) + 1 # updating the annotation pointer\n",
    "\n",
    "        prod_name.append( ' '.join(cont)) # complete phrase for each product\n",
    "        prod_ann.append(ann)\n",
    "        #print(prod_name)\n",
    "        \n",
    "    ## combining content with annotations\n",
    "    prod =[]\n",
    "    for i in range(len(data)):\n",
    "        prod.append([prod_name[i], prod_ann[i]])\n",
    "        \n",
    "    ## creating a dataframe with product names and annotations\n",
    "    prod_data = pd.DataFrame(prod, columns = ['ProdName','Annotations'])\n",
    "    \n",
    "    # converting into csv file\n",
    "    prod_data.to_csv(product+'_trainData.csv', index= None)\n",
    "    \n",
    "    # convert into json\n",
    "    csvfile = open(product+'_trainData.csv', 'r')\n",
    "    jsonfile = open(product+'_trainData.json', 'w')\n",
    "\n",
    "    fieldnames = ('ProdName', 'Annotations')\n",
    "    reader = csv.DictReader( csvfile, fieldnames)\n",
    "\n",
    "    for row in reader:\n",
    "        json.dump(row, jsonfile)\n",
    "        jsonfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to convert json file into spacy traning data format\n",
    "def convert_to_spacytrain(json_file):\n",
    "    try:\n",
    "        training_data = []\n",
    "        lines=[]\n",
    "        with open(json_file, 'r') as f:\n",
    "            lines = f.readlines() \n",
    "            \n",
    "        for line in lines[1:400]: # loop for every product\n",
    "            data = json.loads(line) # single row\n",
    "            text = data['ProdName'] #this is complete phrase\n",
    "            entities = data['Annotations']\n",
    "            training_data.append((text, {\"entities\" : eval(entities)}))\n",
    "            \n",
    "        return training_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        \n",
    "        logging.exception(\"Unable to process \" + json_file + \"\\n\" + \"error = \" + str(e))\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spacy(nlp,data,iterations):\n",
    "    \n",
    "    TRAIN_DATA = data\n",
    "       \n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "                ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "#         if nlp is None:\n",
    "#             optimizer = nlp.begin_training()\n",
    "#         else:\n",
    "#             optimizer = nlp.resume_training()\n",
    "        for itn in range(iterations):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                #print(text, annotations)\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    #drop=0.2,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "                \n",
    "            print(losses)\n",
    "            if (losses['ner'] <100):\n",
    "                break\n",
    "\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mparvatham/DatasetCreation/spacy-ner-annotator-master/Spacy_NER'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## to get current location path\n",
    "path = os.getcwd() \n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/mparvatham/DatasetCreation/spacy-ner-annotator-master/Spacy_NER/catalogue/laptop.csv', '/Users/mparvatham/DatasetCreation/spacy-ner-annotator-master/Spacy_NER/catalogue/swiches.csv', '/Users/mparvatham/DatasetCreation/spacy-ner-annotator-master/Spacy_NER/catalogue/HardDisk.csv', '/Users/mparvatham/DatasetCreation/spacy-ner-annotator-master/Spacy_NER/catalogue/printers.csv', '/Users/mparvatham/DatasetCreation/spacy-ner-annotator-master/Spacy_NER/catalogue/monitors.csv']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = []\n",
    "\n",
    "for file in os.listdir(path+\"/catalogue\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        files.append(os.path.join(path+\"/catalogue\", file))\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prod1\n",
      "Starting iteration 0\n",
      "{'ner': 1475.4946768784332}\n",
      "Starting iteration 1\n",
      "{'ner': 125.85785638970043}\n",
      "Starting iteration 2\n",
      "{'ner': 156.76143061454846}\n",
      "Starting iteration 3\n",
      "{'ner': 542.1258806540089}\n",
      "Starting iteration 4\n",
      "{'ner': 230.27847208136853}\n",
      "Starting iteration 5\n",
      "{'ner': 95.33896605580753}\n",
      "prod2\n",
      "Starting iteration 0\n",
      "{'ner': 748.3494689263875}\n",
      "Starting iteration 1\n",
      "{'ner': 36.19504030366512}\n",
      "prod3\n",
      "Starting iteration 0\n",
      "{'ner': 470.3095202818697}\n",
      "Starting iteration 1\n",
      "{'ner': 0.020973742354482083}\n",
      "prod4\n",
      "Starting iteration 0\n",
      "{'ner': 421.1634865267402}\n",
      "Starting iteration 1\n",
      "{'ner': 235.19775699540472}\n",
      "Starting iteration 2\n",
      "{'ner': 218.32219859684818}\n",
      "Starting iteration 3\n",
      "{'ner': 217.44229563161633}\n",
      "Starting iteration 4\n",
      "{'ner': 170.5059630554129}\n",
      "Starting iteration 5\n",
      "{'ner': 163.94962442145658}\n",
      "Starting iteration 6\n",
      "{'ner': 163.20267752830327}\n",
      "Starting iteration 7\n",
      "{'ner': 151.7358513070732}\n",
      "Starting iteration 8\n",
      "{'ner': 144.01171361637176}\n",
      "Starting iteration 9\n",
      "{'ner': 139.5806510379378}\n",
      "Starting iteration 10\n",
      "{'ner': 121.81003463772008}\n",
      "Starting iteration 11\n",
      "{'ner': 118.41948207695795}\n",
      "Starting iteration 12\n",
      "{'ner': 120.62613843600381}\n",
      "Starting iteration 13\n",
      "{'ner': 111.31342322956908}\n",
      "Starting iteration 14\n",
      "{'ner': 107.2759145696039}\n",
      "Starting iteration 15\n",
      "{'ner': 118.98922059223534}\n",
      "Starting iteration 16\n",
      "{'ner': 107.66358196161771}\n",
      "Starting iteration 17\n",
      "{'ner': 105.36723183996222}\n",
      "Starting iteration 18\n",
      "{'ner': 101.36617136851979}\n",
      "Starting iteration 19\n",
      "{'ner': 84.10864508860884}\n",
      "prod5\n",
      "Starting iteration 0\n",
      "{'ner': 953.9090014068292}\n",
      "Starting iteration 1\n",
      "{'ner': 33.17257661449518}\n"
     ]
    }
   ],
   "source": [
    "count= 1\n",
    "nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "       \n",
    "for file in files:\n",
    "    print('prod'+str(count))\n",
    "    json_file = create_trainData(file, 'prod'+str(count))\n",
    "    train_data = convert_to_spacytrain('prod'+str(count)+'_trainData.json')\n",
    "    nlp = train_spacy(nlp,train_data,20)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_1 =\"HP \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity:  What\n",
      "Details:  0 4 HDMI\n",
      "Entity:  is the\n",
      "Details:  5 11 DisplayType\n",
      "Entity:  cost\n",
      "Details:  12 16 Brand\n",
      "Entity:  of\n",
      "Details:  17 19 DisplayType\n",
      "Entity:  Lenovo\n",
      "Details:  20 26 Brand\n",
      "Entity:  Ideapad\n",
      "Details:  27 34 Category\n",
      "Entity:  ?\n",
      "Details:  34 35 Brand\n"
     ]
    }
   ],
   "source": [
    "#test_text = input(\"Enter your testing text: \")\n",
    "doc = nlp(test_1)\n",
    "\n",
    "for ent in doc.ents:\n",
    "\n",
    "    print('Entity: ',ent.text)\n",
    "    print('Details: ',ent.start_char, ent.end_char, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
