{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to create a NER model for electronic products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The products used are:\n",
    "\n",
    "- Laptops\n",
    "- Monitors\n",
    "- HardDisks\n",
    "- Printers\n",
    "- Shredders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load data\n",
    "def create_trainData (csv_file,product):\n",
    "    \n",
    "    ## loading data with the columns names as entities\n",
    "    data = pd.read_csv(csv_file,index_col=None)\n",
    "    \n",
    "    ## the entities\n",
    "    cols = data.columns\n",
    "    \n",
    "    ## number of entities\n",
    "    num_ent = len(data.columns)\n",
    "    ent_list = list(np.arange(num_ent))\n",
    "    \n",
    "    ## jumble indices of entities to create a phrase\n",
    "    prod_name = [] # list of all product names\n",
    "    prod_ann = [] # list of all the annotations\n",
    "    for i in range(len(data)): # loop for each laptop\n",
    "        idx_list = random.sample(ent_list,num_ent) # shuffling indices\n",
    "        cont = []\n",
    "        ann = []\n",
    "        ann_idx = 0 # pointer for annotating \n",
    "        for j in range(num_ent): # creating the jumbled product name\n",
    "            col_num = idx_list[j] # column number according jumbled column index\n",
    "            val = data.iloc[i,col_num] # value of the entity \n",
    "            cont.append(val) # appending list of entities into a single list\n",
    "            ann.append((ann_idx, len(val)+ ann_idx, cols[col_num])) # annotations and entity name\n",
    "            ann_idx = ann_idx + len(val) + 1 # updating the annotation pointer\n",
    "\n",
    "        prod_name.append( ' '.join(cont)) # complete phrase for each product\n",
    "        prod_ann.append(ann)\n",
    "        \n",
    "    ## combining content with annotations\n",
    "    prod =[]\n",
    "    for i in range(len(data)):\n",
    "        prod.append([prod_name[i], prod_ann[i]])\n",
    "        \n",
    "    ## creating a dataframe with product names and annotations\n",
    "    prod_data = pd.DataFrame(prod, columns = ['ProdName','Annotations'])\n",
    "    \n",
    "    # converting into csv file\n",
    "    prod_data.to_csv(product+'_trainData.csv', index= None)\n",
    "    \n",
    "    # convert into json\n",
    "    csvfile = open(product+'_trainData.csv', 'r')\n",
    "    jsonfile = open(product+'_trainData.json', 'w')\n",
    "\n",
    "    fieldnames = ('ProdName', 'Annotations')\n",
    "    reader = csv.DictReader( csvfile, fieldnames)\n",
    "\n",
    "    for row in reader:\n",
    "        json.dump(row, jsonfile)\n",
    "        jsonfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to convert json file into spacy traning data format\n",
    "def convert_to_spacytrain(json_file):\n",
    "    try:\n",
    "        training_data = []\n",
    "        lines=[]\n",
    "        with open(json_file, 'r') as f:\n",
    "            lines = f.readlines() \n",
    "            \n",
    "        for line in lines[1:400]: # loop for every product\n",
    "            data = json.loads(line) # single row\n",
    "            text = data['ProdName'] #this is complete phrase\n",
    "            entities = data['Annotations']\n",
    "            training_data.append((text, {\"entities\" : eval(entities)}))\n",
    "            \n",
    "        return training_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        \n",
    "        logging.exception(\"Unable to process \" + json_file + \"\\n\" + \"error = \" + str(e))\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spacy(data,iterations):\n",
    "    \n",
    "    TRAIN_DATA = data\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    \n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "       \n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "                ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                #print(text, annotations)\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.2,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "                \n",
    "            print(losses)\n",
    "            if (losses['ner'] <100):\n",
    "                break\n",
    "\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prdnlp = train_spacy(train_data,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = create_trainData('laptop.csv', 'laptop')\n",
    "train_data = convert_to_spacytrain('laptop_trainData.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = convert_to_spacytrain('laptop_trainData.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 0\n",
      "{'ner': 1688.0190589409233}\n",
      "Starting iteration 1\n",
      "{'ner': 337.61964259671106}\n",
      "Starting iteration 2\n",
      "{'ner': 203.93771591093065}\n"
     ]
    }
   ],
   "source": [
    "test = train_spacy(train_data,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = create_trainData('laptop.csv', 'laptop')\n",
    "train_data = convert_to_spacytrain('laptop_trainData.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
